---
title: "IMM reliability"
author: "Attila"
date: '2020 Ã¡prilis 9 '
output:
  html_document:
    toc: true
    theme: united
    number_sections: true
editor_options: 
  chunk_output_type: console
---
```{r include=FALSE}
setwd("D:/Documents/Egyebek/Thesis")
```
# Data input and opening libraries

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(readr); library(tidyverse); library(readxl); library(lubridate); library(ggplot2); library(stringr); library(vroom); library(janitor); library(widyr)
```

# Explicit Measures

Reading in data files
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
round1 <- read_csv("D:/Documents/Egyebek/Thesis/data/Round 1/data.csv")
round2 <- read_csv("D:/Documents/Egyebek/Thesis/data/Round 2/data.csv")
```


## Responses that have not been completed
Extracting responses based on missing completion time ('TIME_end')
```{r}
missing1 <- round1[is.na(round1$TIME_end),]
missing2 <- round2[is.na(round2$TIME_end),]
```

## Uniting round1 and round2 explicit measures
Cleaning variable names, and dropping missing cases. Changing Neptun codes to upper case. Filtering out duplicate answers.
```{r}
time1 <- round1 %>% 
  select(c("participant", "Neptun:1", "TIME_start", "TIME_end")) %>% 
  rename(TIME_start_first = "TIME_start", TIME_end_first = "TIME_end", participant_first = "participant") %>% 
  drop_na()
time2 <- round2 %>% 
  select(c("participant", "Neptun:1", "TIME_start", "TIME_end")) %>% 
  rename(TIME_start_second = "TIME_start", TIME_end_second = "TIME_end", participant_second = "participant") %>% 
  drop_na()

#neptun codes to upper case
time1$`Neptun:1` <- toupper(time1$`Neptun:1`)
time2$`Neptun:1` <- toupper(time2$`Neptun:1`)

#neptun codes more than once in data
neptun_duplicate_time1 <- time1 %>% 
  count(`Neptun:1`) %>% 
  filter(n > 1)
neptun_duplicate_time2 <- time2 %>% 
  count(`Neptun:1`) %>% 
  filter(n > 1)
```

### Joining separate rounds
Cases missing first round means, they either did not complete round 1 or they made a typo in their Neptun code and therefore it can't be automatically matched to it.
```{r}
joined <- left_join(time2, time1, by = "Neptun:1")
missing_joined_first_round <- joined[is.na(joined$TIME_end_first),]
#Neptun codes that appear more than once
neptun_duplicate_joined <- joined %>% 
  count(`Neptun:1`) %>% 
  filter(n > 1)
```

## Creating groups (2 week and 4 week)

### Converting times to date format
```{r}
joined$TIME_end_first <- as.Date(joined$TIME_end_first)
joined$TIME_end_second <- as.Date(joined$TIME_end_second)
joined$TIME_start_first <- as.Date(joined$TIME_start_first)
joined$TIME_start_second <- as.Date(joined$TIME_start_second)
```

### Adding 'days between data collections' column and filtering missing values
```{r}
joined <- joined %>% 
  mutate(
    TIME_between_end = TIME_end_second - TIME_end_first
  ) 
missing_joined_TIME_between <- joined %>% 
  dplyr::filter(is.na("TIME_end_between"))
joined <- joined %>% 
  drop_na(TIME_between_end)
```

### Exporting missing values and checking manually
```{r}
#write_csv2(neptun_duplicate_joined, "D:/Documents/Egyebek/Thesis/data/Temp/neptun_duplicate_joined.csv")
#write_csv2(neptun_duplicate_time1, "D:/Documents/Egyebek/Thesis/data/Temp/neptun_duplicate_time1.csv")
#write_csv2(neptun_duplicate_time2, "D:/Documents/Egyebek/Thesis/data/Temp/neptun_duplicate_time2.csv")
```



### Counting number of participants in groups
The 2 week group is coded as group 0, the 4 week group is coded as group 1.
```{r}
joined$group <- ifelse(joined$TIME_between_end > 20, 1, 0)
# if there are more than 20 days between administration it is assigned 1, else 0
joined %>% 
  count(group)
```

## Plotting frequency of number of days between data collections
```{r}
joined$TIME_between_end <- as.numeric(joined$TIME_between_end)
plot <- ggplot(joined, aes(TIME_between_end))
plot + geom_bar()
```

# Implicit measure

## Importing data
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
#defining path
files1 <- list.files(path = "D:/Documents/Egyebek/Thesis/data/Round 1", pattern = "immtest.*.txt$", full.names = TRUE)
files2 <- list.files(path = "D:/Documents/Egyebek/Thesis/data/Round 2", pattern = "immtest.*.txt$", full.names = TRUE)

#importing data files
gnat_raw1 <- 
  vroom::vroom(file = files1, 
        id = "id", 
        col_names = c("block", "block_id", "word", "max_rt", "trial_type", "word_category", "rt", "error", "target")) %>% 
  extract(col = id, 
          into = c(NA, "id", NA), 
          regex = "^(.*data.)(.*)(.txt)$")
gnat_raw2 <- 
  vroom::vroom(file = files2, 
        id = "id", 
        col_names = c("block", "block_id", "word", "max_rt", "trial_type", "word_category", "rt", "error", "target")) %>% 
  extract(col = id, 
          into = c(NA, "id", NA), 
          regex = "^(.*data.)(.*)(.txt)$")
```

## Data cleaning
### Keeping only important blocks
```{r}
important_targets <- c("chall_pos", "chall_neg", "crit_pos", "crit_neg")

#filter only important trials and only go_trials for first round
gnat_important1 <- 
  gnat_raw1 %>% 
  filter(target %in%important_targets) %>% 
  filter(trial_type == 'go_trial')
#filter only important trials and only go_trials for second round
gnat_important2 <- 
  gnat_raw2 %>% 
  filter(target %in%important_targets) %>% 
  filter(trial_type == 'go_trial')
```

### Establishing block error rate per participant

```{r}
#checking for block error rate above 40% for first round
correct_block_rate1 <- 
  gnat_important1 %>% 
  group_by(id, target) %>% 
  summarise(correct_block = 1 - mean(error))
#checking for block error rate above 40% for second round
correct_block_rate2 <- 
  gnat_important2 %>% 
  group_by(id, target) %>% 
  summarise(correct_block = 1 - mean(error))
```

### Establishing overall error rate per participant

```{r}
#no more removal needed for round 1
correct_all_rate1 <-
  gnat_important1 %>% 
  filter(target %in% important_targets) %>%
  group_by(id) %>% 
  summarise(correct_all = 1 - mean(error))
#no more removal needed for round 2
correct_all_rate2 <-
  gnat_important2 %>% 
  filter(target %in% important_targets) %>%
  group_by(id) %>% 
  summarise(correct_all = 1 - mean(error))
```

### Filtering observations based on error rate 

Criteria: 

* Block error rate below 60%
* Overall error rate below 80%
* Erroneous association
* Response latency under 300 ms
* Response latency $3*SD$ above average RT

```{r}
#must delete these observations due to high block error rate from round1
final_gnat1 <-
  gnat_important1 %>% 
  left_join(correct_block_rate1, by = c("id", "target")) %>% 
  left_join(correct_all_rate1, by = c("id")) %>% 
  filter(correct_block > 0.6 & correct_all > 0.8 & error == 0) %>% 
  filter(rt > 300 & rt < 1161)
#must delete these observations due to high block error rate from round2
final_gnat2 <-
  gnat_important2 %>% 
  left_join(correct_block_rate2, by = c("id", "target")) %>% 
  left_join(correct_all_rate2, by = c("id")) %>% 
  filter(correct_block > 0.6 & correct_all > 0.8 & error == 0) %>% 
  filter(rt > 300 & rt < 1153)
```

Determining average RT and SD for previous filter
```{r}
#deleting rows with too quick and too slow response windows for round1 (too quick is 300 and too slow is 3 SD above averagert ==663+166*3)
final_gnat1 %>% 
  summarise(pers_a = mean(rt))
final_gnat1 %>% 
  summarise(pers_sd = sd(rt))
#deleting rows with too quick and too slow response windows for round2 (too quick is 300 and too slow is 3 SD above averagert ==655+166*3)
final_gnat2 %>% 
  summarise(pers_a = mean(rt))
final_gnat2 %>% 
  summarise(pers_sd = sd(rt))
```

## Rows that have been removed
```{r}
final_filtered1 <- 
  setdiff(final_gnat1, gnat_important1)
```

